<!DOCTYPE html>
<html lang="en">
<head>
  <title></title>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/css/bootstrap.min.css" rel="stylesheet">
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.3.3/dist/js/bootstrap.bundle.min.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>

  <style>
    body{ 
      padding-top: 60px;
    }
    #sidenav{
      position: fixed;
      width: 170px;
      background: #eee;
    }
    #NaiveBayesClassifier, #LogisticRegression, #SVM, #PCA, #CNN, #AutomaticDifferentiation, #References{
      scroll-margin-top: 80px;
      background: #eee;
    }
  </style>
</head>

<body id="BackToTop">

  <nav class="navbar navbar-expand-sm bg-dark navbar-dark fixed-top">
    <ul class="navbar-nav">
      <li class="nav-item">
        <a class="nav-link " href="index.html">Home</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="course.html">Study & Research</a>
      </li>
      <li class="nav-item">
        <a class="nav-link" href="publication.html">Project & Publication</a>
      </li>
       <!-- Dropdown -->
      <li class="nav-item dropdown">
        <a class="nav-link dropdown-toggle active" href="#" role="button" data-bs-toggle="dropdown">More</a>
        <div class="dropdown-menu">
          <a class="dropdown-item" href="forwardModel.html">Forward Model</a>
          <a class="dropdown-item" href="optimization.html">Optimization</a>
          <a class="dropdown-item active" href="machineLearning.html">Machine Learning</a>
        </div>
      </li>
    </ul>
  </nav>
  
  <div class="container-fluid">
    <div class="row">
      <div class="col-sm-2">
        <div class="container p-3 my-3" id="sidenav">
          <a href="#NaiveBayesClassifier">Naive Bayes</a><br>
          <a href="#LogisticRegression">Logistic Regression</a><br>
          <a href="#SVM">SVM</a><br>
          <a href="#PCA">PCA</a><br>
          <a href="#CNN">CNN</a><br>
          <a href="#AutomaticDifferentiation">AutoDifferentiation</a><br>
          <a href="#References">References</a><br>
          <a href="#BackToTop">Back To Top</a>
        </div>
      </div>
      <div class="col-sm-10">
        <div class="container p-3 my-3 border" id="NaiveBayesClassifier">
          <h1>Naive Bayes Classifier</h1>
          <p>
          The naive conditional independence assumption: each feature is (conditionally) independent of every other feature, given the label.
          <br>\( p(x|y) = p(x_1,x_2,...,x_d|y) = \prod\limits_{i=1}^{d}p(x_i|y) \)
          <br>The <b>naive Bayes classifier</b>: the predicted label is given by
          <br>\( \hat{y} = \arg\max\limits_{y} P(y) \prod\limits_{i=1}^{d} p(x_i|y) \)
          <br>The <b>parameters of the classifier</b>:
          <br>\( P(y) \)
          <br>\( p(x_i|y) \) for all \( i,y \)
          <br>
          <br>\( x= \langle x_1, x_2, ..., x_d \rangle \) where each \(x_i\) can take only a finite number of values from \( \{v_1, v_2, ..., v_m\} \)
          <br>In this case, the <b>parameters of the classifier</b> are
          <br>\( P(y) \)
          <br>\( p(x_i=v_k|y) \) for all \( i,k,y \)
          <br>
          <br>Given a training set of n labelled samples \( \langle x^i, y^i \rangle, i = 1,...,n \), how to estimate the model parameters?
          <br>\( P(y) = \frac{\# \: of \: samples \: with \: label \: y}{n} \)
          <br>\( p(x_i=v_k|y)=\frac{p(x_i=v_k,y)}{P(y)} = \frac{\# \: of \: samples \: with \: ith \: feature \: taking \: value \: v_k \: and \: label \: y}{\# \: of \: samples \: with \: label \: y} \)
          </p>
        </div>

        <div class="container p-3 my-3 border" id="LogisticRegression">
          <h1>Logistic Regression</h1>
          <p>
          The <b>logistic function</b>: \( \sigma(t) = \frac{1}{1+\exp(-t)} = \frac{\exp(t)}{1+\exp(t)} \)
          <br>
          <img src="imag/logistic1.png" width="300" height="200">
          <br>
          <br><b>Logistic regression</b>: use the logistic function for modeling \(P(y|x)\), considering only the case of \(y \in \{0,1\}\)
          <br>\( P(y=0|x) = \frac{1}{1+\exp(w_0+\sum_{i=1}^{d}w_ix_i)} = \frac{1}{1+\exp(w^tx)} = 1-\sigma(w^tx) \)
          <br>\( P(y=1|x) = \frac{\exp(w_0+\sum_{i=1}^{d}w_ix_i)}{1+\exp(w_0+\sum_{i=1}^{d}w_ix_i)} = \frac{\exp(w^tx)}{1+\exp(w^tx)} = \sigma(w^tx) \)
          <br>Given a sample x, we classify it as 0 (i.e. predicting y = 0) if 
          <br>\( P(y=0|x) \ge P(y=1|x) \Rightarrow \frac{1}{1+\exp(w^tx)} \ge \frac{\exp(w^tx)}{1+\exp(w^tx)} \Rightarrow w^tx \le 0 \)
          <br>Or we classify it as 1, if \( w^tx > 0 \)
          <br>
          <br>What are the model parameters in logistic regression?
          <br>Given a parameter w, we have \( P(y|x) = \sigma(w^tx)^y(1-\sigma(w^tx))^{1-y} \)
          <br>
          <br>Given n training samples, \( \langle x^i,y^i \rangle, i = 1, ..., n, \) how can we use them to estimate the parameters?
          <br>For a given w, the probability of getting all those \( y^1, y^2, ..., y^n \) from the correponding data \( x^1, x^2, ..., x^n \) is
          <br>\( L(w) = \prod_{i=1}^{n}P(y^i|x^i,w) = \prod_{i=1}^{n}\sigma(w^tx^i)^{y^i}(1-\sigma(w^tx^i))^{1-{y^i}} \)
          <br>\( l(w) = log(L(w)) = \sum_{i=1}^{n}log[\sigma(w^tx^i)^{y^i}(1-\sigma(w^tx^i))^{1-{y^i}}] = \sum_{i=1}^{n}[log(\sigma(w^tx^i)^{y^i}) + log((1-\sigma(w^tx^i))^{1-{y^i}})] \)
          <br>
          <br><b>Optimal parameters?</b>
          <br>\( w^* = \arg\max\limits_{w} l(w) = \arg\max\limits_{w} \sum_{i=1}^{n} [y^iw^tx^i - log(1+\exp(w^tx^i))]  \)
          <br>We can not really solve for \( w^* \) analytically (no closed-form solution), but we can use a commanly-used optimization technique, gradient descent/ascent, to find a solution.
          </p>
        </div>

        <div class="container p-3 my-3 border" id="SVM">
          <h1>SVM (Support Vector Machine)</h1>
          <p>
          <h3>Formulating the Margin</h3>
          We can have the canonical formulation for the three planes as
          <br>\( H: w^t x + b = 0 \)
          <br>\( H_1: w^t x + b = +1 \)
          <br>\( H_2: w^t x + b = -1 \)
          <br> The region between \( H_1 \) and \( H_2 \) is also called the margin, and its width is \( \frac{2}{||w||} \)
          <br>
          <img src="imag/svm1.png" width="300" height="290">
          <br>
          <br>
          <h3>Formulating SVM</h3>
          \( \{ w^*,b^* \} = \arg\min\limits_{w,b}\frac{1}{2}||w||^2 \)
          <br>subject to 
          <br>\( w^tx^i + b \ge +1 \) for \( y^i = +1 \)
          <br>\( w^tx^i + b \le -1 \) for \( y^i = -1 \)
          <br>The constraints can be combined into:
          <br>\( y^i(w^t x^i + b) - 1 \ge 0 \)
          <br> A nonlinear (quadratic) optimization problem with linear inequality constraints.
          <h3>Reformulating using Lagrangian multipliers</h3>
          <b>Lagrangian Primal Form</b>
          <br> \( L_P(w,b,\alpha) = \frac{1}{2}||w||^2 - \sum\limits_{i} \alpha_i[y^i(w^tx^i+b)-1] \)
          <br> then the SVM solution should satisfy
          <br> \( \frac{\partial L_P}{\partial w} = 0, \frac{\partial L_P}{\partial b} = 0, \alpha_i \ge 0, \alpha_i[y^i(w^tx^i+b)-1] = 0 \)
          <br> The final w is given by \( w = \sum\limits_{i} \alpha_i y^i x^i \) and b is given by \( b = y^k - w^t x^k \) for any k such that \( \alpha_k > 0 \)
          <br> <b>Lagrangian Dual Form</b>
          <br> \( L_D(w,b,\alpha) = \sum\limits_{i} \alpha_i - \frac{1}{2}\sum\limits_{i,j} \alpha_i \alpha_j y^i y^j \langle x^i,x^j \rangle, \) where \(\langle x^i,x^j \rangle\) means inner product.
          <br> The solution is the same as before. But there is an important observation.
          <br> Points for which \( \alpha_i > 0 \) are called support vectors.
          </p>
        </div>
        
        <div class="container p-3 my-3 border" id="PCA">
          <h1>PCA (Principal Component Analysis)</h1>
          <p>
          The problem is to <b>find the direction of the largest variance</b>.
          <br> Given n samples \( D=\{x_1,x_2,...,x_n\} \) in d-dimensional space, 
          find a direction \( e_1 \), such that the projection of \( D \) onto \( e_1 \) gives
          the largest variance (compared with any other direction).
          \( e_1 \) is a d-dimensional vector with unit norm.
          <br> The mean of the projections: \( \bar{y} = \frac{1}{n} \sum\limits_{i=1}^{n} y_i = \frac{1}{n} \sum\limits_{i=1}^{n} \langle x_i,e \rangle = \langle \bar{x},e \rangle \)
          <br> The variance of the projections: \( \sigma^2 = \frac{1}{n} \sum\limits_{i=1}^{n} (y_i-\bar{y})^2 = \frac{1}{n} \sum\limits_{i=1}^{n} [\langle x_i-\bar{x},e \rangle]^2 \) 
          <br> Expand the previous expression
          <br> \( \sigma^2 = \sum\limits_{j=1}^{d} \sum\limits_{k=1}^{d} e_j e_k [ \frac{1}{n} \sum\limits_{i=1}^{n} (x_{i,j}-\bar{x}_j) (x_{i,k}-\bar{x}_k) ] = \sum\limits_{j=1}^{d} \sum\limits_{k=1}^{d} e_j e_k C_{jk} = e^t C e \)
          <br> <b>C is the sample covariance matrix</b>.
          <br> To find \( e_1 \), we can do
          <br> \( e_1 = \arg\max\limits_{e} \sigma^2 = e^t C e \) subject to \( ||e|| = 1 \)
          <br> Constrained maximization: use Lagrangian multiplier method.
          <br> maximize \( F(e) = e^t C e - \lambda (e^t e - 1) \)
          <br> Set the partial derivative to 0, we have
          <br> \( \frac{\partial F}{\partial e} = 2 C e - 2 \lambda e = 0 \)
          <br> \( C e = \lambda e \)
          <br> The solution is an eigenvector of \( C \), with eigenvalue \( \lambda \), which is also the variance under \( e \):
          <br> \( \sigma^2 = e^t C e = \lambda \)
          <br> We should set \( e_1 \) to be the eigenvector correponding to the largest eigenvalue \( \lambda_1 \)
        </p>
        </div>

        <div class="container p-3 my-3 border" id="CNN">
          <h1>CNN (Convolutional Neural Network)</h1>
          <p>
          <b>Image Filtering via Convolution: Kernel</b>
          <br> By varying kernel's coefficients, we can achieve different goals - smoothing, sharpening, detecting edges, etc.
          <br> Better yet: can we learn proper kernels? Part of CNN's objective
          <br>
          <br>
          <img src="imag/cnn1.png" width="600" height="230">
          <br> Some conv layers plus some fully-connected layers
          <br> (conv layer: convolution, batchnormalization, activation, pooling)
<pre>
<b>Neural Network Definition Example:</b>
import torch.nn as nn
class Model(nn.Module):<span style="color:blue">
    ## init function is the constructor and we define all the layers used in our model</span>
    def __init__(self, num_classes=10):
        super(Model, self).__init__()<span style="color:blue">
        ''' 
        REST OF THE MODEL HERE
        # define a convolutional layer with 16 channels, kernel_size=3, stride=1 and padding=1
        # define a batchnormalization layer 
        # define a relu layer
        # define a maxpool layer with kernel_size=2, stride=2
        # define a convolutional layer with 32 channels, kernel_size=3, stride=1 and padding=1
        # define a batchnormalization layer
        # define a relu layer
        # define a maxpool layer with kernel_size=2, stride=2
        # define a convolution layer with 64 channels, kernel_size=5, stride=1 and padding=2
        # define a batchnorm layer
        # define a relu layer
        # define a maxpool layer with kernel_size=2, stride=2
        # define a fully connected layer from resulting dimension -> number of classes
        '''</span>
        self.relu=nn.ReLU()
        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv3 = nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2)
        self.bn3 = nn.BatchNorm2d(64)
        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.fc4 = nn.Linear(64*3*3, 10) 
    def forward(self, x):<span style="color:blue">
        # We will start with feeding the data to the first layer 
        # We take the output x and feed it back to the next layer</span>
        x = self.conv1(x)
        x = self.bn1(x)<span style="color:blue">
        # Continue in this manner to get the output of the final layer</span>
        ...
</pre>
          </p>  
        </div>

        <div class="container p-3 my-3 border" id="AutomaticDifferentiation">
          <h1>Automatic Differentiation</h1>
          <p>
          <h3>Computation Graph</h3>
          <img src="imag/autodiff1.png" width="500" height="80">
          <br>
          <br>
          From the perspective of software, instead of using function notation we usually see, it is convenient to use graph notation for describing a neural-network-architecture.
          A neural-network-layer should be an object instantiated from a predefined neural-network-layer class, which has encapsulated useful methods to automatically compute outputs given inputs.
          Then a neural-network-architecture is constructed by stacking these modular neural-network-layer objects together.
          Forward-pass mode (from left to right) is going to automatically compute the activations \(a_1...a_L\)
		  then evaluate the loss function \(loss(a_L,y)\),
		  while backward-pass mode (from right to left) is going to automatically compute the first-derivatives \(\frac{\partial L}{\partial W_1},\frac{\partial L}{\partial b_1}...\frac{\partial L}{\partial W_L},\frac{\partial L}{\partial b_L}\)
          then update the model parameters \(W_1,b_1...W_L,b_L\).
          In the end, the neural-network model is trained by optimizing the model parameters through some gradient-based iterative algorithm.
          <br>
          <br>
          <h3>Modular Network Layer</h3>
          <img src="imag/autodiff2.png" width="280" height="130">
          <br>
          Parameter: \( \{W_1,b_1...W_L,b_L\} \)
          <br>
          Gradient: \( \{\frac{\partial L}{\partial W_1},\frac{\partial L}{\partial b_1}...\frac{\partial L}{\partial W_L},\frac{\partial L}{\partial b_L}\} \)
          <br>
          Cache: \( \{a_1...a_L\} \)
          </p>
        </div>

        <div class="container p-3 my-3 border" id="References">
          <h1>References</h1>
          <a href="https://scikit-learn.org/stable/" target="_blank">Scikit-learn: Machine Learning in Python</a>
          <br>
          <a href="https://pytorch.org/" target="_blank">PyTorch: Deep Learning in Python</a>
        </div>
        
      </div>
    </div>
  </div>

</body>

</html>